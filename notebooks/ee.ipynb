{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "# import pyhanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 0\n",
    "min_count = 1\n",
    "char_size = 128\n",
    "maxlen = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，排除“其他”类型\n",
    "D = pd.read_csv('../data/cat_train.csv', encoding='utf-8', header=None)\n",
    "print(len(D))\n",
    "D = D[D[2] != u'其他']\n",
    "D = D[D[1].str.len() <= maxlen]\n",
    "print(len(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../temp/classes.json'):\n",
    "    id2class = dict(enumerate(D[2].unique()))\n",
    "    class2id = {j:i for i,j in id2class.items()}\n",
    "    json.dump([id2class, class2id], open('../temp/classes.json', 'w'))\n",
    "else:\n",
    "    id2class, class2id = json.load(open('../temp/classes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for t,c,n in zip(D[1], D[2], D[3]):\n",
    "    start = t.find(n)\n",
    "    if start != -1:\n",
    "        train_data.append((t, c, n))\n",
    "    else:\n",
    "        train_data.append((t, c, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "if not os.path.exists('../temp/all_chars_me.json'):\n",
    "    chars = {}\n",
    "    for d in tqdm(iter(train_data)):\n",
    "        items = re.split(\"[\\{\\}]\", d[0])\n",
    "        for item in items:\n",
    "            if item.startswith('DATE') or item.startswith('NUM'):\n",
    "                c = '{'+item+'}'\n",
    "                chars[c] = chars.get(c, 0) +1\n",
    "            else:\n",
    "                for index, c in enumerate(item):\n",
    "                    chars[c] = chars.get(c, 0) + 1\n",
    "    chars = {i:j for i,j in chars.items() if j >= min_count}\n",
    "    id2char = {i+2:j for i,j in enumerate(chars)} # 0: mask, 1: padding\n",
    "    char2id = {j:i for i,j in id2char.items()}\n",
    "    json.dump([id2char, char2id], open('../temp/all_chars_me.json', 'w'))\n",
    "else:\n",
    "    id2char, char2id = json.load(open('../temp/all_chars_me.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id['{NUM_0}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(random_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_data:\n",
    "    if t[2].startswith(\"no_\"):\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../temp/ee_random_order_train.json'):\n",
    "    random_order = list(range(len(train_data)))\n",
    "    np.random.shuffle(random_order)\n",
    "    json.dump(\n",
    "        random_order,\n",
    "        open('../temp/ee_random_order_train.json', 'w'),\n",
    "        indent=4\n",
    "    )\n",
    "else:\n",
    "    random_order = json.load(open('../temp/ee_random_order_train.json'))\n",
    "\n",
    "\n",
    "dev_data = [train_data[j] for i, j in enumerate(random_order) if i % 9 == mode]\n",
    "train_data = [train_data[j] for i, j in enumerate(random_order) if i % 9 != mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv('../data/cat_eval.csv', encoding='utf-8', header=None)\n",
    "test_data = []\n",
    "for id,t,c in zip(D[0], D[1], D[2]):\n",
    "    test_data.append((id, t, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=64):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X, C, S1, S2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                text = d[0]\n",
    "                x = [char2id.get(c, 1) for c in text]\n",
    "                c = class2id[d[1]]\n",
    "                s1, s2 = np.zeros(len(text)), np.zeros(len(text))\n",
    "                start = text.find(d[2])\n",
    "                end = start + len(d[2]) - 1\n",
    "                s1[start] = 1\n",
    "                s2[end] = 1\n",
    "                X.append(x)\n",
    "                C.append([c])\n",
    "                S1.append(s1)\n",
    "                S2.append(s2)\n",
    "                if len(X) == self.batch_size or i == idxs[-1]:\n",
    "                    X = seq_padding(X)\n",
    "                    C = seq_padding(C)\n",
    "                    S1 = seq_padding(S1)\n",
    "                    S2 = seq_padding(S2)\n",
    "                    yield [X, C, S1, S2], None\n",
    "                    X, C, S1, S2 = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"多头注意力机制\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.out_dim = nb_head * size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        q_in_dim = input_shape[0][-1]\n",
    "        k_in_dim = input_shape[1][-1]\n",
    "        v_in_dim = input_shape[2][-1]\n",
    "        self.q_kernel = self.add_weight(name='q_kernel',\n",
    "                                        shape=(q_in_dim, self.out_dim),\n",
    "                                        initializer='glorot_normal')\n",
    "        self.k_kernel = self.add_weight(name='k_kernel',\n",
    "                                        shape=(k_in_dim, self.out_dim),\n",
    "                                        initializer='glorot_normal')\n",
    "        self.v_kernel = self.add_weight(name='w_kernel',\n",
    "                                        shape=(v_in_dim, self.out_dim),\n",
    "                                        initializer='glorot_normal')\n",
    "    def mask(self, x, mask, mode='mul'):\n",
    "        if mask is None:\n",
    "            return x\n",
    "        else:\n",
    "            for _ in range(K.ndim(x) - K.ndim(mask)):\n",
    "                mask = K.expand_dims(mask, K.ndim(mask))\n",
    "            if mode == 'mul':\n",
    "                return x * mask\n",
    "            else:\n",
    "                return x - (1 - mask) * 1e10\n",
    "    def call(self, inputs):\n",
    "        q, k, v = inputs[:3]\n",
    "        v_mask, q_mask = None, None\n",
    "        if len(inputs) > 3:\n",
    "            v_mask = inputs[3]\n",
    "            if len(inputs) > 4:\n",
    "                q_mask = inputs[4]\n",
    "        # 线性变化\n",
    "        qw = K.dot(q, self.q_kernel)\n",
    "        kw = K.dot(k, self.k_kernel)\n",
    "        vw = K.dot(v, self.v_kernel)\n",
    "        # 形状变换\n",
    "        qw = K.reshape(qw, (-1, K.shape(qw)[1], self.nb_head, self.size_per_head))\n",
    "        kw = K.reshape(kw, (-1, K.shape(kw)[1], self.nb_head, self.size_per_head))\n",
    "        vw = K.reshape(vw, (-1, K.shape(vw)[1], self.nb_head, self.size_per_head))\n",
    "        # 维度置换\n",
    "        qw = K.permute_dimensions(qw, (0, 2, 1, 3))\n",
    "        kw = K.permute_dimensions(kw, (0, 2, 1, 3))\n",
    "        vw = K.permute_dimensions(vw, (0, 2, 1, 3))\n",
    "        # Attention\n",
    "        a = K.batch_dot(qw, kw, [3, 3]) / self.size_per_head**0.5\n",
    "        a = K.permute_dimensions(a, (0, 3, 2, 1))\n",
    "        a = self.mask(a, v_mask, 'add')\n",
    "        a = K.permute_dimensions(a, (0, 3, 2, 1))\n",
    "        a = K.softmax(a)\n",
    "        # 完成输出\n",
    "        o = K.batch_dot(a, vw, [3, 2])\n",
    "        o = K.permute_dimensions(o, (0, 2, 1, 3))\n",
    "        o = K.reshape(o, (-1, K.shape(o)[1], self.out_dim))\n",
    "        o = self.mask(o, q_mask, 'mul')\n",
    "        return o\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = Input(shape=(None,)) # 待识别句子输入\n",
    "c_in = Input(shape=(1,)) # 事件类型\n",
    "s1_in = Input(shape=(None,)) # 实体左边界（标签）\n",
    "s2_in = Input(shape=(None,)) # 实体右边界（标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, c, s1, s2 = x_in, c_in, s1_in, s2_in\n",
    "x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Embedding(len(id2char)+2, char_size)(x)\n",
    "c = Embedding(len(class2id), char_size)(c)\n",
    "c = Lambda(lambda x: x[0] * 0 + x[1])([x, c])\n",
    "\n",
    "x = Add()([x, c])\n",
    "x = Dropout(0.2)(x)\n",
    "x = Lambda(lambda x: x[0] * x[1])([x, x_mask])\n",
    "\n",
    "x = Bidirectional(CuDNNLSTM(char_size//2, return_sequences=True))(x)\n",
    "x = Lambda(lambda x: x[0] * x[1])([x, x_mask])\n",
    "x = Bidirectional(CuDNNLSTM(char_size//2, return_sequences=True))(x)\n",
    "x = Lambda(lambda x: x[0] * x[1])([x, x_mask])\n",
    "\n",
    "xo = x\n",
    "x = Attention(8, 16)([x, x, x, x_mask, x_mask])\n",
    "x = Lambda(lambda x: x[0] + x[1])([xo, x])\n",
    "\n",
    "x = Concatenate()([x, c])\n",
    "\n",
    "x1 = Dense(char_size, use_bias=False, activation='tanh')(x)\n",
    "ps1 = Dense(1, use_bias=False)(x1)\n",
    "ps1 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10)([ps1, x_mask])\n",
    "\n",
    "x2 = Dense(char_size, use_bias=False, activation='tanh')(x)\n",
    "ps2 = Dense(1, use_bias=False)(x2)\n",
    "ps2 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10)([ps2, x_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([x_in, c_in], [ps1, ps2])\n",
    "\n",
    "\n",
    "train_model = Model([x_in, c_in, s1_in, s2_in], [ps1, ps2])\n",
    "\n",
    "loss1 = K.mean(K.categorical_crossentropy(s1_in, ps1, from_logits=True))\n",
    "loss2 = K.mean(K.categorical_crossentropy(s2_in, ps2, from_logits=True))\n",
    "loss = loss1 + loss2\n",
    "\n",
    "train_model.add_loss(loss)\n",
    "train_model.compile(optimizer=Adam(1e-3))\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity(text_in, c_in):\n",
    "    \"\"\"解码函数，应自行添加更多规则，保证解码出来的是一个公司名\n",
    "    \"\"\"\n",
    "    if c_in not in class2id:\n",
    "        return 'NaN'\n",
    "    _x = list()\n",
    "    items = re.split(\"[\\{\\}]\", text_in)\n",
    "    for item in items:\n",
    "        if item.startswith('DATE') or item.startswith('NUM'):\n",
    "            c = '{'+item+'}'\n",
    "            _x.append(char2id.get(c, 1))\n",
    "        else:\n",
    "            for c in item:\n",
    "                _x.append(char2id.get(c, 1))\n",
    "    \n",
    "    _x = [char2id.get(c, 1) for c in text_in]\n",
    "    _x = np.array([_x])\n",
    "    _c = np.array([[class2id[c_in]]])\n",
    "    _ps1, _ps2  = model.predict([_x, _c])\n",
    "    start = _ps1[0].argmax()\n",
    "    end = _ps2[0][start:].argmax() + start\n",
    "    return text_in[start: end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Callback):\n",
    "    def __init__(self):\n",
    "        self.ACC = []\n",
    "        self.best = 0.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = self.evaluate()\n",
    "        self.ACC.append(acc)\n",
    "        if acc > self.best:\n",
    "            self.best = acc\n",
    "            train_model.save_weights('best_model.weights')\n",
    "        print('acc: %.4f, best acc: %.4f\\n' % (acc, self.best))\n",
    "    def evaluate(self):\n",
    "        A = 1e-10\n",
    "        for d in tqdm(iter(dev_data)):\n",
    "            R = extract_entity(d[0], d[1])\n",
    "            if R == d[2]:\n",
    "                A += 1\n",
    "        return A / len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data):\n",
    "    \"\"\"注意官方页面写着是以\\t分割，实际上却是以逗号分割\n",
    "    \"\"\"\n",
    "    with open('../temp/result.txt', 'w', encoding='utf-8') as F:\n",
    "        for d in tqdm(iter(test_data)):\n",
    "            s = '\"%s\",\"%s\"\\n' % (d[0], extract_entity(d[1].replace('\\t', ''), d[2]))\n",
    "            F.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_data:\n",
    "    if not t[2]:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluate()\n",
    "train_D = data_generator(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.fit_generator(train_D.__iter__(),\n",
    "                          steps_per_epoch=len(train_D),\n",
    "                          epochs=2000,\n",
    "                          callbacks=[evaluator]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.save(\"../models/ccks.20191202.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entity(\"截至 {DATE_0}，公司已通过股票回购专用证券账户以集中竞价交易方式回购股份数量 {NUM_0} 股，占公司总股本的 {NUM_1}，最高成交价为 {NUM_2} 元/股，最低成交价为 {NUM_3} 元/股\", \n",
    "               \"回购__总金额\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        matrix1 = tf.constant([[3., 3.]])\n",
    "        matrix2 = tf.constant([[2.], [2.]])\n",
    "        product = tf.matmul(matrix1, matrix2)\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
